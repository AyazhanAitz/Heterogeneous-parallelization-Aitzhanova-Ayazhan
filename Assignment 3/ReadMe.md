## Задача 1. Поэлементная обработка массива (Global и Shared memory)

### Цель

Реализовать CUDA-программу для поэлементного умножения массива на число и сравнить две версии:

1. с использованием только глобальной памяти;
2. с использованием разделяемой памяти.

### Реализация

В первой версии каждый поток напрямую читает и записывает данные в глобальной памяти GPU.
Во второй версии данные сначала загружаются в разделяемую память блока, затем выполняется умножение, после чего результат записывается обратно в глобальную память.
Для измерения времени выполнения обеих реализаций использовались CUDA-события (cudaEvent).

### Результаты

Программа была запущена для массива размером 1 000 000 элементов.
По результатам замеров было получено:

- Global memory: 0.113 ms, корректность: OK
- Shared memory: 0.047 ms, корректность: OK

Таким образом, версия с использованием разделяемой памяти показала меньшее время выполнения по сравнению с версией, использующей только глобальную память. Разница во времени составляет более чем в 2 раза, при этом корректность вычислений в обеих версиях подтверждена.

### Вывод

В данной программе использование разделяемой памяти позволило повысить производительность CUDA-программы. Это связано с тем, что доступ к shared memory осуществляется значительно быстрее, чем к глобальной памяти, и в данной конфигурации выигрыш от быстрого доступа перекрыл накладные расходы на копирование данных и синхронизацию потоков.

---

## Задача 2. Поэлементное сложение массивов и влияние размера блока

### Цель

Исследовать влияние размера блока потоков на производительность CUDA-ядра при поэлементном сложении двух массивов.

### Реализация

Каждый поток CUDA выполняет поэлементное сложение одного элемента массивов.
Размер массива составлял 1 000 000 элементов.
Для анализа производительности были протестированы различные размеры блока потоков: 64, 128, 256 и 512.

Для повышения точности измерений время выполнения усреднялось по нескольким запускам ядра.
Дополнительно рассчитывалась приблизительная пропускная способность памяти (Bandwidth).

### Результаты

В ходе выполнения программы были получены следующие значения:

- blockSize = 64. Время выполнения: 0.0511 ms, корректность: OK, пропускная способность: ≈ 234.88 GB/s
- blockSize = 128. Время выполнения: 0.0494 ms, корректность: OK, пропускная способность: ≈ 243.01 GB/s
- blockSize = 256. Время выполнения: 0.0493 ms, корректность: OK, пропускная способность: ≈ 243.43 GB/s
- blockSize = 512. Время выполнения: 0.0495 ms, корректность: OK, пропускная способность: ≈ 242.51 GB/s

Минимальное время выполнения было достигнуто при размере блока 256 потоков.
При увеличении или уменьшении размера блока относительно этого значения наблюдается небольшое ухудшение производительности.

### Вывод

Результаты подтверждают, что размер блока потоков оказывает заметное влияние на производительность CUDA-программы.
Слишком малый размер блока приводит к менее эффективной загрузке GPU, тогда как оптимальный размер блока (в данном случае 128–256 потоков) обеспечивает максимальную пропускную способность памяти и минимальное время выполнения.

Таким образом, корректный выбор размера блока позволяет повысить производительность CUDA-ядра без изменения алгоритма вычислений.

---

## Задача 3. Коалесцированный и некоалесцированный доступ к памяти

### Цель

Продемонстрировать влияние шаблона доступа к глобальной памяти на производительность CUDA-программы.

### Реализация

Была реализована CUDA-программа, выполняющая поэлементную операцию над массивом размером **1 000 000 элементов**.
Были реализованы две версии ядра:

* **Коалесцированный доступ** — потоки одного варпа обращаются к последовательным адресам глобальной памяти.
* **Некоалесцированный доступ** — потоки обращаются к данным с большим шагом (stride), что приводит к разрозненным обращениям к памяти.

Для измерения времени выполнения использовались CUDA-события (`cudaEvent`). Корректность результатов проверялась после каждого запуска.

### Результаты

По результатам выполнения программы были получены следующие значения:

* **Coalesced доступ**: 0.0382 ms, корректность: OK
* **Non-coalesced доступ**: 0.0875 ms, корректность: OK

Отношение времени выполнения составило:

* **Non-coalesced / Coalesced = 2.2883**

Это означает, что версия с некоалесцированным доступом выполняется более чем в **2 раза медленнее**, чем версия с коалесцированным доступом.

### Вывод

Результаты наглядно показывают, что шаблон доступа к глобальной памяти оказывает существенное влияние на производительность CUDA-программы. Коалесцированный доступ обеспечивает значительно более высокую скорость выполнения, так как обращения потоков одного варпа объединяются в меньшее число транзакций глобальной памяти.

Некоалесцированный доступ приводит к увеличению количества обращений к памяти и, как следствие, к заметному росту времени выполнения программы.

---

## Задача 4. Подбор оптимальной конфигурации сетки и блоков потоков

### Цель

Подобрать оптимальную конфигурацию сетки и блоков потоков для одной из реализованных CUDA-программ и сравнить её с неоптимальной.

### Реализация

Была реализована программа поэлементного сложения массивов на GPU.
Размер массива составлял **N = 1 000 000 элементов**.
Были протестированы различные размеры блока потоков: **32, 64, 128, 256, 512 и 1024**.

В качестве неоптимальной конфигурации был выбран малый размер блока **32 потока**, так как он, как правило, хуже загружает вычислительные ресурсы GPU.
Оптимальная конфигурация определялась на основе минимального среднего времени выполнения CUDA-ядра.

---

### Результаты

По результатам программы были получены следующие значения времени выполнения:

* blockSize = 32 → avg_time = 0.1046 ms
* blockSize = 64 → avg_time = 0.0505 ms
* blockSize = 128 → avg_time = 0.0493 ms
* blockSize = 256 → avg_time = 0.0493 ms
* blockSize = 512 → avg_time = 0.0495 ms
* blockSize = 1024 → avg_time = 0.0506 ms

Корректность вычислений для всех конфигураций подтверждена (check = OK).

В ходе сравнения конфигураций:

* **Неоптимальная конфигурация**: blockSize = 32, avg_time = 0.0954 ms
* **Оптимальная конфигурация**: blockSize = 256, avg_time = 0.0491 ms

Полученное ускорение составило **1.94 раза**.

---

### Вывод

Результаты показывают, что размер блока потоков существенно влияет на производительность CUDA-программы. При слишком малом размере блока GPU используется неэффективно, что приводит к увеличению времени выполнения.

Подбор оптимальной конфигурации блоков и сетки позволил почти в два раза сократить время выполнения программы без изменения алгоритма, что подтверждает важность настройки параметров запуска CUDA-ядра для повышения производительности.


