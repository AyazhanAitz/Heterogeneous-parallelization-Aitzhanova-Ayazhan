## Тема: Оптимизация параллельного кода на GPU с использованием различных типов памяти

## 1. Цель работы

Целью данной работы является изучение влияния различных типов памяти GPU на производительность параллельных вычислений, а именно:

* реализация редукционного алгоритма вычисления суммы элементов массива на GPU;
* сравнение производительности двух подходов:

  * редукция с использованием **только глобальной памяти (global memory)**;
  * редукция с использованием **комбинации глобальной и разделяемой памяти (shared memory)**;
* анализ зависимости времени выполнения от размера входного массива.

## 2. Описание задачи

В работе реализована программа на **CUDA C++**, выполняющая следующие этапы:

1. Генерация одномерного массива случайных целых чисел.
2. Вычисление суммы элементов массива на GPU:

   * вариант A — редукция с использованием только global memory;
   * вариант B — редукция с использованием global + shared memory.
3. Замер времени выполнения каждого варианта с использованием `cudaEvent`.
4. Сравнение результатов и анализ влияния использования разделяемой памяти.
5. Экспорт результатов в файл `results.csv` для последующего построения графиков.

## 3. Используемые технологии

* Язык программирования: **C++17**
* Платформа параллельных вычислений: **NVIDIA CUDA**
* Типы памяти GPU:

  * Global memory
  * Shared memory
* Инструменты измерения времени:

  * `cudaEvent_t`

## 4. Теоретическая часть

### 4.1 Global memory

Глобальная память GPU:

* имеет большую ёмкость;
* обладает высокой латентностью;
* доступна всем потокам и блокам;
* при использовании атомарных операций (`atomicAdd`) может вызывать **сильную конкуренцию потоков**.

### 4.2 Shared memory

Разделяемая память:

* находится на уровне одного блока;
* имеет очень низкую латентность;
* используется для обмена данными между потоками одного блока;
* позволяет значительно сократить количество обращений к global memory.

## 5. Полученные результаты

Результаты измерений времени выполнения редукции суммы:

| Размер массива (N) | Global memory, ms | Global + Shared, ms |
| -----------------: | ----------------: | ------------------: |
|             10 000 |           59.0333 |            0.004096 |
|            100 000 |          0.002048 |            0.002048 |
|          1 000 000 |          0.002464 |            0.002272 |


## 6. Анализ и объяснение результатов

### 6.1 Случай N = 10 000

Для небольшого массива наблюдается **резкое замедление варианта с использованием только global memory**.

Причины:

* каждый поток выполняет `atomicAdd` в одну и ту же ячейку глобальной памяти;
* возникает высокая конкуренция между потоками;
* атомарные операции в global memory имеют высокую задержку.

В варианте с использованием shared memory:

* потоки сначала суммируют данные внутри блока;
* атомарная операция выполняется **один раз на блок**, а не на поток;
* количество атомарных операций резко сокращается;
* время выполнения уменьшается на несколько порядков.


### 6.2 Случай N = 100 000

Для среднего размера массива времена выполнения практически совпадают.

Причины:

* вычисления становятся более равномерно распределёнными;
* накладные расходы на запуск ядра и синхронизацию доминируют над временем вычислений;
* эффект от оптимизации памяти становится менее заметным.


### 6.3 Случай N = 1 000 000

Для большого массива вариант с shared memory снова показывает **небольшое преимущество**.

Причины:

* увеличивается объём вычислений;
* снижается относительное влияние накладных расходов;
* уменьшение числа атомарных операций даёт стабильный выигрыш по времени.


## 7. Выводы

В ходе выполнения работы были получены следующие выводы:

1. Использование только глобальной памяти при редукции приводит к высокой конкуренции потоков из-за атомарных операций.
2. Применение разделяемой памяти позволяет:

   * уменьшить количество обращений к global memory;
   * снизить число атомарных операций;
   * повысить производительность редукционного алгоритма.
3. Эффект оптимизации наиболее заметен:

   * при малых и больших размерах данных;
   * при высокой степени параллелизма.
4. Для средних размеров массивов влияние оптимизации памяти может быть сглажено накладными расходами GPU.
