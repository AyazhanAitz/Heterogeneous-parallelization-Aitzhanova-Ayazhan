# Отчет 
## Анализ производительности редукции и префиксной суммы с использованием CUDA

---

## 1. Теоретическая часть

### 1.1. Редукция (Reduction)

Редукция — это операция агрегации элементов массива в одно итоговое значение с использованием некоторой ассоциативной операции, чаще всего сложения. Примером редукции является вычисление суммы, максимума, минимума или произведения элементов массива.

В параллельных вычислениях редукция широко используется, например, при:

* подсчёте статистик (сумма, среднее значение);
* нормализации данных;
* вычислении скалярных произведений;
* агрегировании результатов параллельных вычислений.

На GPU редукция реализуется иерархически: сначала частичные суммы вычисляются внутри блоков потоков (обычно с использованием разделяемой памяти), затем результаты блоков редуцируются повторно до одного значения.

---

### 1.2. Префиксная сумма (Scan)

Префиксная сумма (scan) — это операция, при которой каждый элемент выходного массива равен сумме всех предыдущих элементов входного массива (включительно или исключительно).

**Инклюзивный scan:**
[ out[i] = a[0] + a[1] + ... + a[i] ]

**Эксклюзивный scan:**
[ out[i] = a[0] + a[1] + ... + a[i-1] ]

Префиксные суммы применяются в:

* параллельной сортировке;
* построении гистограмм;
* графовых алгоритмах;
* компактации массивов;
* динамическом программировании.

В CUDA scan обычно реализуется в несколько этапов: локальный scan внутри блока с использованием shared memory, затем обработка сумм блоков и добавление смещений.

---

## 2. Практическая часть

### 2.1. Реализация алгоритмов

В рамках работы были реализованы следующие алгоритмы:

1. **Редукция на CPU** — последовательное суммирование элементов массива.
2. **Редукция на GPU (atomic)** — использование `atomicAdd` в глобальной памяти.
3. **Редукция на GPU (shared memory)** — оптимизированная версия с локальной редукцией внутри блоков.
4. **Префиксная сумма на CPU** — последовательный инклюзивный scan.
5. **Префиксная сумма на GPU** — много-блочная реализация с использованием разделяемой памяти.

Для ускорения передачи данных дополнительно использовалась **pinned host memory**, что позволило сократить время копирования данных между CPU и GPU.

(Исходный код приведён в приложении / в файле `task3_perf.cu`.)

---

### 2.2. Условия тестирования

* Тип данных: `float`
* Размеры массивов:

  * 2¹⁰ (1 024)
  * 2¹⁵ (32 768)
  * 2¹⁸ (262 144)
  * 2²⁰ (1 048 576)
  * 2²² (4 194 304)
* Размер блока CUDA: 256 потоков
* Замер времени:

  * CPU: `std::chrono`
  * GPU: `cudaEvent`

Корректность результатов проверялась сравнением с CPU-реализацией с допустимой погрешностью для чисел с плавающей точкой.

---

### 2.3. Результаты тестирования

На основе экспериментальных измерений были построены графики зависимости времени выполнения алгоритмов от размера массива (см. рисунки ниже).

### 2.3.1. Редукция (Reduction)

По графику редукции видно следующее:

* **CPU-реализация** демонстрирует почти линейный рост времени выполнения с увеличением размера массива. При малых размерах (2¹⁰) CPU работает очень быстро за счёт отсутствия накладных расходов.
* **GPU редукция с использованием atomicAdd (global memory)** показывает наихудшую производительность. С ростом размера массива время резко увеличивается из‑за большого количества конфликтующих атомарных операций.
* **GPU редукция с использованием shared memory** является наиболее эффективной. Время выполнения растёт значительно медленнее по сравнению с CPU и atomic‑версией, особенно на больших массивах (2¹⁸–2²²).

Таким образом, использование разделяемой памяти позволяет существенно сократить количество обращений к глобальной памяти и повысить производительность редукции.

### 2.3.2. Префиксная сумма (Scan)

Анализ графика префиксной суммы показывает:

* **CPU scan** имеет линейную сложность и быстро становится узким местом при увеличении размера массива. Уже при размерах выше 2¹⁸ время выполнения превышает 1 мс и продолжает резко расти.
* **GPU scan** демонстрирует более плавный рост времени выполнения. Несмотря на накладные расходы на запуск ядра, при больших размерах массива GPU существенно опережает CPU.

Преимущество GPU‑реализации особенно заметно на массивах размером 2²⁰ и выше, где достигается кратное ускорение по сравнению с CPU.

---

### 2.4. Графики производительности

Были построены следующие графики:

1. **Время редукции vs размер массива** (CPU, GPU atomic, GPU shared)
2. **Время scan vs размер массива** (CPU vs GPU)
3. **Время копирования данных** (pageable vs pinned)

По графикам видно, что при малых размерах массива накладные расходы на запуск GPU-ядра превышают выигрыш, однако при больших объёмах данных GPU значительно превосходит CPU.

---

## 3. Выводы

### 3.1. Анализ результатов

Экспериментальные результаты подтверждают, что эффективность параллельных алгоритмов напрямую зависит от способа организации памяти и характера доступа к данным. На малых объёмах данных накладные расходы GPU нивелируют преимущества параллелизма, однако при увеличении размера массива GPU начинает значительно выигрывать.

---

### 3.2. Сравнение производительности CPU и GPU

* Для **малых массивов (2¹⁰ – 2¹⁵)** CPU показывает сопоставимую или лучшую производительность.
* Для **средних и больших массивов (≥ 2¹⁸)** GPU существенно превосходит CPU как для редукции, так и для префиксной суммы.
* Использование **atomicAdd** в глобальной памяти приводит к резкому падению производительности и не рекомендуется для масштабируемых решений.
* Оптимизированные GPU‑алгоритмы с **shared memory** обеспечивают наилучшее соотношение скорости и масштабируемости.

---

### 3.3. Рекомендации по оптимизации

На основе проведённого анализа можно сформулировать следующие рекомендации:

1. Использовать **shared memory** для локальных вычислений внутри блоков.
2. Избегать частого применения **atomic операций** в глобальной памяти.
3. Применять **иерархические алгоритмы** (multi‑block) для больших массивов.
4. Использовать **pinned host memory** для ускорения передачи данных между CPU и GPU.
5. Подбирать оптимальный размер блока потоков (обычно 128–256) для достижения максимальной загрузки GPU.

---

## Заключение

В ходе работы были реализованы и проанализированы алгоритмы редукции и префиксной суммы на CPU и GPU. На основе экспериментальных данных и построенных графиков показано, что GPU обеспечивает значительное ускорение при обработке больших массивов данных, особенно при использовании оптимизированных схем доступа к памяти. Полученные результаты подтверждают целесообразность применения CUDA для задач массовой обработки данных.
